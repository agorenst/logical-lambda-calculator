\documentclass{article}
\usepackage{noweb}
\usepackage{minted}
\usepackage{syntax}
\usepackage{hyperref}
\usepackage{cleveref}

\title{Logical Lambda Calculator \\ \large Part 1}
\author{Aaron Gorenstein}
\date{January 1, 2021}

\begin{document}

\maketitle

\begin{abstract}
I wanted to explore different models of computation.
I explored how might we realize $\lambda$-calculus through the ``engine'' of first-order predicate logic.
The motivation for this document was a study on some excellent books\cite{lambdacalc,artofprolog} on both these topics.
What value this study provides is largely credited to those books; what errors found here are my own fault.

The original purpose of this document was to try to create a really definitive exploration.
In concession to real-world constraints, I reduced the scope to more like personal notes--the exposition is not as exhaustive as I would have liked.
\end{abstract}

\section{Introduction}
\paragraph{Broad Thoughts}
The $\lambda$-calculus is a famous and fundamental model of computation.
It realizes computation through \emph{function application}.
First-order predicate-logic is another equally fundamental model of computation.
In the language prolog, it enables computation through \emph{unification}.

To be clear: These two models of computation realize computation through different mechanisms.
In the coarsest terms, $\lambda$-calculus iteratively applies variable substitution (we shall see this ultimately as $\beta$-reduction) to produce a quiescent $\lambda$-term; this represents the result of the computation.
For the first-order predicate-logic case, we will search in a depth-first-search-like manner through a possible database of facts to find the most general answer to a query. Whatever this search finds--or that it cannot find an answer--represents the result of the computation.

By implementing the means of evaluating $\lambda$-calculus expressions in prolog, we shall hopefully gain a deeper insight--or at least appreciation--for the differences here.

\paragraph{Reader Beware}
The meta-history of this document is, I believe, I wrote the major code in early 2016. Sometime Fall 2016 I added comments and uploaded it to the online repository Github.
In late 2020, I resurrected this document and decided to fully document the code in a literate coding sense.
Some of the details of prolog, especially, may be misremembered, though obviously the root approach and code all works up to the tests.

\section{Representing $\lambda$-Calculus in Prolog}
The grammar for $\lambda$-calculus\cite{lambdacalc} is strikingly self-contained:
\begin{grammar}
<expression> ::= <name> \alt <function> \alt <application>

<function> ::= $\lambda$ <name> . <expression>

<application> ::= ( <expression> <expression> )
\end{grammar}
This can be copied almost verbatim into our prolog code:
<<Initial Lambda Calculus Definitions>>=
expression(L) :- name(L).
expression(L) :- function(L).
expression(L) :- application(L).
function([lambda, V, B]) :- name(V), expression(B).
application([E1, E2]) :- expression(E1), expression(E2).
name(X) :- not(is_list(X)), ground(X), X \= lambda.
@
Observe that we also obligate ourselves to define [[name(X)]], not present in the grammar.

We can demonstrate how these clauses work.
Prolog can help verify that the following sentences are true:
<<Initial1.tests>>=
name(x).
function([lambda, a, a]).
application([a, b]).
application([[lambda, a, a], b]).
application([[lambda, a, a], [lambda, c, [lambda, c, c]]]).
@


\subsection{Implementing $\beta$-Reduction}
The fundamental action of $\lambda$-calculus is surely the $\beta$-reduction, whereby all instances of the bound variable is replaced by the applicant of the function.
We define this as:
<<Beta Reduction>>=
beta_reduction([lambda, V, B], A, R) :- replace(V, B, A, R).
@
Read as English, we would say: ``the $\beta$ reduction of [[F]], when applied to [[A]], results in [[R]].
This is true when [[F]] is some function with bound variable [[V]] and a body [[B]], and [[R]] is the result of replacing all instances of [[V]] in [[B]] with [[A]].''
By coarse analogy to imperative languages: the first term of the sentence is both type-checking [[F]] that it is a function, as well as accessing its fields [[V]] and [[B]].
The second term essentially has the out-parameter [[R]], the result of replacing [[V]] in [[B]] with [[A]].\footnote{Note
I may be shaky on the vocabulary here, ``bound'' variable and ``term'' and so on.}

Let us see how [[replace]] is realized in prolog:
<<Replace Predicate>>=
replace(V,V,A,A) :- name(V).
replace(V,W,_,W) :- name(W), V \= W.
replace(V,[lambda, V, B],_,[lambda, V, B]).
replace(V,[lambda, W, B],A,[lambda, W, S]) :- W \= V,
    replace(V,B,A,S).
replace(V, [E1, E2], A, [R1, R2]) :-
    replace(V,E1,A,R1),
    replace(V,E2,A,R2).
@
We shall consider each sentence in turn.

The first definition is the core replacement: our body [[B]] \emph{is} the bound variable [[B]], and so we should set our out-parameter [[R]] to be exactly [[A]].
This ``setting'' is of course not what happens in prolog.
Rather it should be understood as saying that, when [[V]] is a name, a valid replacement is when [[V]] and [[B]] are the same, and [[A]] and [[R]] are the same.
The second definition captures the case when the body of the function is a variable [[W]], but one that is \emph{not} the same as our bound variable [[V]].
In that case, the result is that no replacement should take place: we unify the result with the (unchanged) body [[W]].

These two definitions play a role in the following examples of $\beta$-reduction:
<<BetaReductions.tests>>=
beta_reduction([lambda, x, x], a, a).
beta_reduction([lambda, x, y], a, y).
@
Observe how the first case, when we ``get to'' the replacement clause, [[V=x]] and [[B=x]]. Consequently, we set [[R]] to be the applicant [[a]].
In the second case, there is no such matching ([[V \= W]]), and so [[R]] is set to the unchanged body [[y]].
These two lines are literally explicit queries one can ask prolog--the expectation is that prolog should report ``yes'' for both as we have implemented everything correctly.

So we have tested all possible cases when the body of our function is simply a name, and we can turn to other cases.
We can see that there are two cases when we the body of function is itself a nested function.
The first is when our nested function \emph{itself} defines the symbol [[V]].
That is, we have a $\lambda$ term like: $\lambda x.\lambda.x(x y)$.
We have the expectation that something like, say, $\lambda x.\lambda x (x y) a b \to (b y)$, rather than $(a y)$.
This is ``shadowing'', and it is realized in this third definition by simply\ldots not replacing anything.

We can realize this in tests like so:
<<BetaReductions.tests>>=
beta_reduction([lambda, x, [lambda, x, [x, y]]], a, [lambda, x, [x, y]]).
@

The second definition for when our second parameter is a function is interesting.
We recursively replace all instances inside the \emph{body} of [[F]], [[B]], and this yields us a new expression [[S]].
We use this result [[S]] to compose the body of the resulting, upper-level function.

<<BetaReductions.tests>>=
beta_reduction([lambda, x, [lambda, y, [x, y]]], a, [lambda, y, [a, y]]).
@

Lastly, applications, lacking any immediate name context, are easy.

We can make sure these work in higher-level contexts:
<<BetaReductions.tests>>=
beta_reduction([lambda, x, x], [lambda, y, y], R), R == [lambda, y, y].
beta_reduction([lambda, x, [x, z]], [lambda, y, y], R), R == [[lambda, y, y], z].
@

\subsection{Evaluation}
The $\beta$-reduction is the fundamental action through which we define what it is to \emph{evaluate} a $\lambda$-expression.
Let us do that now:
<<Evaluate>>=
evaluate(L, L) :- name(L).
evaluate(L, L) :- function(L).
evaluate([E1, E2], R) :-
    beta_reduction(E1, E2, R1),
    evaluate(R1, R).
evaluate([E1, E2], R) :-
    evaluate(E1, R1),
    evaluate([R1, E2], R).
@
Similar to before, we consider each possible ``type'' of $\lambda$-expression.
The evaluation of a name, or a function, leaves those types unchanged.
It is only when we encounter an \emph{application} is the result of evaluation not (necessarily) equal to the input.
When the applicator (the left-hand-side of the application pair, i.e., [[E1]]) is a name, there is nothing for us to do.
If the applicator is a function--hooray, this is the entry point into the $\beta$-reduction and we continue.
If the applicator itself is an application pair (something like $((a b) c)$, we first need to evaluate the pair $(a b)$ to yield [[R1]], and then recurse with that updated applicator.

Observe the tail-recursive definitions of [[evaluate]] for the [[function(E1)]] and [[application(E1)]] cases.

Observe also that we are doing \emph{normal-order} evaluation, instead of \emph{applicative-order} evaluation.
This can be demonstrated with the following claim:
<<EvaluationTests.tests>>=
evaluate([ [lambda, x, [lambda, y, [y, x]]], [[lambda, a, [a, a]], b] ] , [lambda, y, [y, [[lambda, a, [a, a]], b]]]).
@
Observe how we don't evaluate the self-apply function.

In a sense, we have achieved our goal.
We have a (rather awkward) way of writing arbitrary lambda expressions and evaluate them.

By analogy, I would suggest we have something like the ALU in a CPU.
We can do the fundamental operations we're interested in, but lack memory or any reasonable interface to actually deploy this computation.
The following section we introduce an \emph{extremely} bare-bones environment so that we may more easily introduce interesting computation.
There is still quite a distance to, say, Scheme (and indeed, we are not going to be getting there),
but this will contain a few more interesting applications of prolog to illuminate interesting things in $\lambda$-calculus.

\section{Introducing an Environment}
The simplest environment would be to ``save'' an atom, such as [[identity]], and associate it with a value, such as [[[lambda, x, x]]].
In cases where we see the (unbound) atom [[identity]], we would replace it with the value.
As this does not truly add any computational power, this sort of machinery is conventionally called ``sugaring'' (and its removal, ``desugaring'').

<<Desugaring>>=
desugar([N,E], L, R) :- beta_reduction([lambda, N, L], E, R).
desugar_all(L, D, R) :- foldl(desugar, D, L, R).
@

TODO: include a demonstration that this doesn't quite do what we want--we will need to ``resugar'' our result.

TODO: demonstrate we need $\alpha$-conversion, too.

We want to map the resugaring, bottom-up, over the tree.
We are simply reversing beta reduction:
<<Resugaring>>=
resugar([N, E], L, N) :- isomorphic(L, E).
resugar(M, [lambda, V, B], [lambda, V, SB]) :- resugar(M, B, SB).
resugar(M, [E1, E2], [R1, R2]) :-
    resugar(M, E1, R1),
    resugar(M, E2, R2).
resugar(_, L, L).

resugar_all(L, D, R) :- foldl(resugar, D, L, S), S \= L, resugar_all(S, D, R).
resugar_all(L, _, L).
@

<<Alpha Reduction>>=
alpha_reduction(L, L) :- name(L).
alpha_reduction([E1, E2], [R1, R2]) :-
    alpha_reduction(E1, R1),
    alpha_reduction(E2, R2).
alpha_reduction(L, [lambda, X, ABB]) :- function(L),
    gensym(alpha_,X),
    beta_reduction(L, X, BB),
    alpha_reduction(BB, ABB).

canon(L, R) :- reset_gensym(alpha_), alpha_reduction(L, R).

isomorphic(A, B) :- canon(A, C), canon(B, C).
@

<<AlphaReduction.tests>>=
isomorphic([lambda, x, x], [lambda, y, y]).
@

<<FirstLoop.filetest>>=
(define true (lambda x (lambda y x)))
(define false (lambda x (lambda y y)))
(define and (lambda x (lambda y ((x y) false))))
(define not (lambda x ((x false) true)))
(define nand (lambda x (lambda y (not ((and x) y)))))
(execute ((nand true) true))
(execute ((nand true) false))
(execute ((nand false) true))
(execute ((nand false) false))
(halt)
@

<<SecondLoop.filetest>>=
(define true (lambda x (lambda y x)))
(define false (lambda x (lambda y y)))
(define and (lambda x (lambda y ((x y) false))))
(define not (lambda x ((x false) true)))
(define nand (lambda x (lambda y (not ((and x) y)))))
(execute nand)
(execute (nand true))
(execute (lambda x (lambda y (((not nand) x) y))))
(define nnand (lambda x (lambda y (not ((nand x) y)))))
(execute (lambda x (lambda y (not ((nand x) y)))))
(execute ((nnand true) true))
(execute ((nnand true) false))
(execute ((nnand false) true))
(execute ((nnand false) false))
(halt)
@

NOTE: Putting ``not'' in front of ``nand'' evaluates things in the wrong order, things aren't commutative like I'd like.

Now let's show that ``not nand'' is equal to ''and''.

\section{Parsing}
We have been looking at $\lambda$s in an annoying way, as [[[lambda, N, B]]].
It would be nice if we could write that as [[(lambda N B)]], for instance.
We need a parser.

NOTE: This is an example where not all ``sugar'' can be implemented within $\lambda$-calculus.
Obviously here we need I/O hooks, but later on we can't really excise parens except by writing a different parser (which we will not do).

In classic parsing style, we'll implement a tokenizer, and then a grammar.

\subsection{Tokenizer}
Handling input streams is tricky and annoying.
If memory serves (this is me writing in 2020 trying to remember what I did in 2016) I am greatly indebted to Sterling and Shapiro\cite{artofprolog} in designing a prolog-implemented parser.

<<Tokenizer>>=
read_s(S) :-           get_char(C),                    read_s(C, S).
read_s(' ',S)       :- get_char(D),                    read_s(D, S).
read_s('(',['('|S]) :- get_char(D),                    read_s(D, S).
read_s(')',[')'|S]) :- get_char(D),                    read_s(D, S).
read_s(C,[N|S])     :- name_char(C), read_name(C,N,D), read_s(D, S).
read_s('\n',[]).
@
The tokenizer is a driver that reads in new characters, and matches them into tokens: parens, names, or end-of-line.
The clause [[get_char]] is the entry point into the outside world.
Again think of the non-chronological perspective. It's almost like we're building up [[S]].
Observe that we use the \emph{goal} to basically append to [[S]].

The output, in a sense, is [[S]] in the topmost clause.

The variable [[C]] is something like the ``current'' character, and [[D]] is the ``next'' character.
We have to keep these characters in the parameter lists so that we don't accidentally discard them.
Observe, for instance, how [[end_of_line]] is a non-recursive case.

The only ``interesting'' token, then, is that determined by [[read_name]].
The clauses for that predicate are as follows:
<<Read Name>>=
read_name(C,N,E) :-
    read_name_chars(S,C,E),
    atom_chars(N,S).
read_name_chars([C|S], C, E) :-
    name_char(C), !, % needed for io
    get_char(D),
    read_name_chars(S, D, E).
read_name_chars([],C,C) :- not(name_char(C)).

name_char(C) :- char_code(C,N), N >= 65, N =< 90. % upper case
name_char(C) :- char_code(C,N), N >= 97, N =< 122. % lower case.
name_char(C) :- char_code(C,N), N == 95. % underscore
name_char(C) :- char_code(C,N), N >= 48, N =< 57. % decimals
@
The output parameter, such that it is, is [[N]] in the first clause.
That is an atom, resulted by the character [[C]] \emph{prepended} on to the string [[S]] built up by [[read_name_chars]].
The character [[E]] is the evidence that we've stopped reading valid-name-characters (see the last clause of [[read_name_chars]]).

I can't explain the need for [[!]], except the comment included from 2016.

This concludes the tokenizer.

\subsection{Parser}
Now given a sequence of tokens, we can distill them into the nested lists we want for our internal representation.
The tricky thing is matching parentheses.
<<Parser>>=
parse2(['(', lambda, X | T], [lambda, X, B]) :-
    name(X),
    append(S, [')'], T),
    parse2(T, B).
parse([')'|T], [], [')'|T]).
parse(['('|T], [M|L], R) :- parse(T,M,[')'|S]), parse(S,L,R).
parse([N|T], [N|L], R) :- N \= ')', N \= '(', parse(T,L,R).
parse([],[],[]).
parse(S,L) :- parse(S,L,[]).
@
How does this work? That's an excellent question.
The entry point is obviously the last line.
The last case, [[parse/2]], is our entry point: it parses the string [[S]] into the $\lambda$-expression [[L]].
Observe that it requires we go through all of [[parse/3]] and end with the 3rd parameter as an empty list.
Let's explore the semantics of [[parse/3]].

The first parameter is some prefix of the original (tokenized) input, the middle parameter is the $\lambda$-expression representing that prefix, and the third parameter is the unparsed suffix of the input.
This explains the final sentence as well as the [[parse/3]] base case: we are done when there is no unparsed suffix left.

For the middle definition, [[parse([N|T], [N|L], R)]], this is a ``simple case''.
If the token [[N]] is not a parenthesis, then it's simply some name in the $\lambda$-expression, and we append it on as appropriate.

For the second definition, [[parse(['('|T], [M|L], R)]], we have a tricky interaction.
We have some internal term [[T]] presumably ``inside'' the parens we're starting.
The parse of that [[T]] is [[M]], characterized by when we find a suffix [[S]] that begins with [[')']].
This [[M]] is simply appended on to the larger expression [[L]], which is the tail-call of parsing that suffix.
(Note the similarity of that tail-call machinery to the ``name'' case below.)

Finally, the first sentence is another base case: when we've reached the [['C']], we should ``end'' that $\lambda$ expression with an empty list (upon which, ultimately, all the actual terms will be appended).

We also want to print out our internal lambda representation the same way.
Note that [[atomic_list_concat]] is provided by our environment, and does what you think.
<<Lambda Printer>>=
lambda_to_atom(L, L) :- name(L).
lambda_to_atom([lambda, V, B], R) :-
    lambda_to_atom(B, BS),
    atomic_list_concat(['(lambda ', V, ' ', BS, ')'], '', R).
lambda_to_atom([E1, E2], R) :-
    lambda_to_atom(E1, R1),
    lambda_to_atom(E2, R2),
    atomic_list_concat(['(', R1, ' ', R2, ')'], '', R).
@

Finally, we want to present a simple user-interactive loop.
Don't call it a repl.
We have a few different commands we can unify against:
<<Repl Commands>>=
execute_command(['define', N, L], D, [[N,L]|D]).
execute_command(['execute', L], D,D) :-
    desugar_all(L, D, DL),
    evaluate(DL, R),
    resugar_all(R,D,SR),
    lambda_to_atom(SR,O),
    write(O), nl.
execute_command(['halt'],_,_) :- halt.
@

TODO: Parsing lambda strings back and forth -- couldn't that be a single 3-parameter clause?
Similar with sugaring/desugaring. The whole point of prolog is the magic fact that there's no time.


<<Main Loop>>=

main_loop(OD) :-
    read_s(S),
    parse(S,[L]), % I strip out the other list immediately. Why not.
    execute_command(L,OD,ND),
    main_loop(ND).
main_loop(OD) :- write('Parse error'), nl, main_loop(OD).

% :- initialization main.
main :- main_loop([]), halt.
@



<<lambda.pl>>=
<<Initial Lambda Calculus Definitions>>
<<Beta Reduction>>
<<Replace Predicate>>
<<Evaluate>>
<<Alpha Reduction>>
<<Desugaring>>
<<Resugaring>>
<<Tokenizer>>
<<Read Name>>
<<Parser>>
<<Lambda Printer>>
<<Repl Commands>>
<<Main Loop>>
@

<<Evaluate.tests>>=
evaluate([[lambda, x, x], [lambda, x, x]], R), R == [lambda, x, x].
evaluate([[lambda, x, [x, x]], [lambda, x, x]], R), R == [lambda, x, x].
@

\section{Extentions}
Resugaring should be able to be expressed as beta-reduction, but backwards. This may require canonicalization to be done in beta reductions.

Observe that we very often traverse the $\lambda$-expression tree (the repeated pattern of the name being the base case, and the function/applications being recursively mapped).
Perhaps this suggests a second-order predicate that maps, or folds, or both, over the expression.
Maybe this is too close to $\lambda$-calculus and would get us full circle; I don't know.

I wonder if the parser can be improved or reduced in lines-of-code.

\appendix
\section{Complete Code Listings}
\subsection{lambda.pl}
\inputminted[linenos=true]{prolog}{lambda.pl}

\bibliography{library}
\bibliographystyle{alpha}

\end{document}