\documentclass{article}
\usepackage{noweb}
\usepackage{minted}
\usepackage{syntax}
\usepackage{hyperref}
\usepackage{cleveref}
% \usepackage{fullpage}

\title{Logical Lambda Calculator}
\author{Aaron Gorenstein}
\date{January 1, 2021}

\begin{document}

\maketitle

\begin{abstract}
I wanted to explore different models of computation.
I explored how might we realize $\lambda$-calculus through the ``engine'' of first-order predicate logic.
The motivation for this document was a study on some excellent books\cite{lambdacalc,artofprolog} on both these topics.
What value this study provides is largely credited to those books; what errors found here are my own fault.

The original purpose of this document was to try to create a really definitive exploration.
In concession to real-world constraints, I reduced the scope to more like personal notes--the exposition is not as exhaustive as I would have liked.
\end{abstract}

\section{Introduction}
\paragraph{Broad Thoughts}
The $\lambda$-calculus is a famous and fundamental model of computation.
It realizes computation through \emph{function application}.
First-order predicate-logic is another equally fundamental model of computation.
In the language prolog, it enables computation through \emph{unification}.

To be clear: These two models of computation realize computation through different mechanisms.
In the coarsest terms, $\lambda$-calculus iteratively applies variable substitution (we shall see this ultimately as $\beta$-reduction) to produce a quiescent $\lambda$-term; this represents the result of the computation.
For the first-order predicate-logic case, we will search in a depth-first-search-like manner through a possible database of facts to find the most general answer to a query. Whatever this search finds--or that it cannot find an answer--represents the result of the computation.

By implementing the means of evaluating $\lambda$-calculus expressions in prolog, we shall hopefully gain a deeper insight--or at least appreciation--for the differences here.

\paragraph{Reader Beware}
The meta-history of this document is, I believe, I wrote the major code in early 2016. Sometime Fall 2016 I added comments and uploaded it to the online repository Github.
In late 2020, I resurrected this document and decided to fully document the code in a literate coding sense.
Some of the details of prolog, especially, may be misremembered, though obviously the root approach and code all works up to the tests.

\section{Representing $\lambda$-Calculus in Prolog}
The grammar for $\lambda$-calculus\cite{lambdacalc} is strikingly self-contained:
\begin{grammar}
<expression> ::= <name> \alt <function> \alt <application>

<function> ::= $\lambda$ <name> . <expression>

<application> ::= ( <expression> <expression> )
\end{grammar}
This can be copied almost verbatim into our prolog code:
<<Initial Lambda Calculus Definitions>>=
expression(L) :- name(L).
expression(L) :- function(L).
expression(L) :- application(L).
function([lambda, V, B]) :- name(V), expression(B).
application([E1, E2]) :- expression(E1), expression(E2).
name(X) :- not(is_list(X)), ground(X), X \= lambda.
@
What is striking--and this is a tool distinct in Prolog--is how the structure of, e.g.,  a function is on the left-hand side of the predicate.
We describe the structure we expect, and then add further requirements on the right-hand side.

Perhaps an analogy is in functional-language-esque pattern-matching, but by the ``bidirectionality'' available in prolog it can be used to both break apart expressions, and build new ones.

\subsection{Demonstrations}
The following sentences should be true:
<<Structure.tests>>=
name(x).
name(y).
function([lambda, a, a]).
application([a, b]).
function([lambda, a, [a, b]]).
application([[lambda, a, a], b]).
application([[lambda, a, a], [lambda, c, [lambda, c, c]]]).
not(function([a, b])).
not(function([lambda, a, b, c])).
not(application([lambda, a, b])).
not(name([a])).
@

\section{Implementing $\beta$-Reduction}
The fundamental action of $\lambda$-calculus is surely the $\beta$-reduction, whereby all instances of the bound variable is replaced by the applicant of the function.
We define this as:
<<Beta Reduction>>=
beta_reduction([lambda, V, B], A, R) :- replace(V, B, A, R).
@
In English: The $\beta$-reduction of a $\lambda v.b$ applied to $A$ is $R$, where $R$ is $B$, but with all instances of $V$ replaced by $A$.
More immediately: we go through the body $B$ and do the text-substitution.

Let us see how [[replace]] is realized in prolog:
<<Replace Predicate>>=
replace(V,V,A,A) :- name(V).
replace(V,W,_,W) :- name(W), V \= W.
replace(V,[lambda, V, B],_,[lambda, V, B]).
replace(V,[lambda, W, B],A,[lambda, W, S]) :-
    W \= V,
    replace(V,B,A,S).
replace(V, [E1, E2], A, [R1, R2]) :-
    replace(V,E1,A,R1),
    replace(V,E2,A,R2).
@
We shall consider each rule in turn.
\begin{enumerate}
\item This is a nice demonstration of ``the magic of prolog''. Here the first parameter is [[V]], \emph{and} the expression we want to replace is [[V]].
In other words, we've found the name we want to replace with [[A]].
Consequently, the fourth parameter is the same as [[A]], i.e., we ``do'' the substitution.
Truth be told, we don't need the requirement ``name'', but it's probably a good idea.
\item This is a similar situation, but the body [[W]] is \emph{not} the same as the variable we're substituting. In that case, there's nothing to rename, so the out-parameter is set to [[W]] as well.
\item The body [[B]] is some function that \emph{puns} (shadows?) the bound variable.
In this case we shouldn't do any replacement, so we completely ignore the third parameter, and the fourth is our body unchanged.
\item The body [[B]] is some function that \emph{doesn't} shadow the bound varaible. Our output is the same function, but with the interior body [[B]] changed to [[S]], which is the recursive call.
\item Lastly, this is when we have an appliction: \emph{this} case doesn't change which variables are or aren't bound, so it's a ``straightforward'' recursive definition.
Note in particular how the output parameter ``already uses'' [[R1]] and [[R2]], which feels unintuitive for those accustomed to imperative or functional languages.
\end{enumerate}

Hopefully this is understood as a fairly concise and immediate definition of how replacements of $\beta$-reductions are realized in $\lambda$-calculus.
To further elucidate, here are some $\beta$-reductions demonstration some of the naming behavior.
Observe that we \emph{must} have a function as the first parameter.
\subsection{Demonstrations}
<<BetaReductions.tests>>=
beta_reduction([lambda, x, x], a, a).
beta_reduction([lambda, x, y], a, y).
beta_reduction([lambda, x, [lambda, x, [x, y]]], a, [lambda, x, [x, y]]).
beta_reduction([lambda, x, [lambda, y, [x, y]]], a, [lambda, y, [a, y]]).
beta_reduction([lambda, x, x], [lambda, x, [x, x]], [lambda, x, [x, x]]).
@

\section{Evaluation}
The complete computation for $\lambda$-calculus requires \emph{evaluation}.
Where $\beta$-reductions are only defined for functions, evaluation will take any $\lambda$-expression and give the result.
There is applicative order and normal order--we shall do normal.
<<Evaluate>>=
evaluate([E1, E2], R) :-
    evaluate(E1, R1), E1 \= R1,
    evaluate([R1, E2], R).
evaluate([E1, E2], R) :-
    beta_reduction(E1, E2, R1),
    evaluate(R1, R).
evaluate(L, L).
@
For conciseness the implementation of this procedure uses recursion, and exploits that prolog will always search for the \emph{first} solution.
\begin{enumerate}
\item The first rule shows that when considering an application, we should first evaluate its left-hand side (in case if the left-hand side is \emph{also} an application).
\item The second rule will eagerly try to do a beta-reduction (which will fail is [[E1]] is not a function) and recurse on the result.
\item The third succinctly capture that, if we are no longer able to evolve the $\lambda$-expression, we're done.
\end{enumerate}
This definition of evaluation opens the door to multiple answers, essentially each one doing ``less and less'' computation.
We order of rules in the procedure means that the first answer always has the maximal amount of computation.
See chapter 7 of \cite{artofprolog}.

\subsection{Demonstrations}
<<Evaluation.tests>>=
evaluate(a, a).
evaluate([lambda, a, a], [lambda, a, a]).
evaluate([[lambda, a, a], b], b).
<<Normal Order Tests>>
@
We can confirm we have normal-order evaluation:
<<Normal Order Tests>>=
evaluate([ [lambda, x, [lambda, y, [y, x]]], [[lambda, a, [a, a]], b] ] , [lambda, y, [y, [[lambda, a, [a, a]], b]]]).
@
Observe how we don't evaluate the self-apply function.

In a sense, we have achieved our goal.
We have a (rather awkward) way of writing arbitrary lambda expressions and evaluate them.

By analogy, I would suggest we have something like the ALU in a CPU.
We can do the fundamental operations we're interested in, but lack memory or any reasonable interface to actually deploy this computation.
The following section we introduce an \emph{extremely} bare-bones environment so that we may more easily introduce interesting computation.
There is still quite a distance to, say, Scheme (and indeed, we are not going to be getting there),
but this will contain a few more interesting applications of prolog to illuminate interesting things in $\lambda$-calculus.

\section{Introducing an Environment}
The simplest environment would be to ``save'' an atom, such as [[identity]], and associate it with a value, such as [[[lambda, x, x]]].
In cases where we see the (unbound) atom [[identity]], we would replace it with the value.
As this does not truly add any computational power, this sort of machinery is conventionally called ``sugaring'' (and its removal, which I'll call ``desugaring'').

Classic definitions for, e.g., logical values include:
<<Logic Definitions>>=
(define true (lambda x (lambda y x)))
(define false (lambda x (lambda y y)))
(define and (lambda x (lambda y ((x y) false))))
(define not (lambda x ((x false) true)))
@
A fuller exploration of these can be found in \cite{lambdacalc}.

Assuming we have those names mapping, we would like to replace those names with their associated expressions before evaluation.
Of course, $\lambda$-calculus machinery is excellent at replacing names with other values:
<<Desugaring>>=
desugar([N,E], L, R) :- beta_reduction([lambda, N, L], E, R).
desugar_all(L, D, R) :- foldl(desugar, D, L, R).
@
So given a sugar-using $\lambda$ expression, we can now succinctly take out the sugar, opening the door for further evaluation.
However, the \emph{result} of that computation would be unsugared.
So an expression like [[((and false) true)]], which we would hope give us [[false]], would in fact give us [[lambda, x, [lambda, y, y]]].
We would like resugar those results.
In spirit, this is ``merely'' doing beta-reduction in reverse:
<<Resugaring>>=
resugar([N, E], L, N) :- isomorphic(L, E).
resugar(M, [lambda, V, B], [lambda, V, SB]) :- resugar(M, B, SB).
resugar(M, [E1, E2], [R1, R2]) :-
    resugar(M, E1, R1),
    resugar(M, E2, R2).
resugar(_, L, L).
resugar_all(L, D, R) :- foldl(resugar, D, L, S), S \= L, resugar_all(S, D, R).
resugar_all(L, _, L).
@
Consider each rule:
\begin{enumerate}
\item The first rule is the main machinery: if the $\lambda$-expression [[L]] is isomorphic to the expression [[E]] that is sugared as name [[N]], we should output [[N]].
\item Rules 2 and 3 are the recursive exploration of the $\lambda$-expression.
\item[4.] Rule 4 ensures that we always succeed at resugaring, even if we fail to do any actual substitution.
\item[5.] We will maximally apply resugaring, and then if there was any change try again.
\item[6.] Again we ensure that we'll always succeed at resugaring even if no changes were detected.
\end{enumerate}
Maybe there is a more prolog-y way of doing this?

\section{The $\alpha$-Reduction}
Of course if we have a macro for [[lambda, y, y]], we would want to it to ``match'' against even if [[L]] is [[lambda, x, x]].
That is why we have the (cryptic) clause [[isomorphic]] as part of the resugaring procedure.
This is an excellent reason to introduce $\alpha$-reduction:
<<Alpha Reduction>>=
alpha_reduction(L, L) :- name(L).
alpha_reduction([E1, E2], [R1, R2]) :-
    alpha_reduction(E1, R1),
    alpha_reduction(E2, R2).
alpha_reduction([lambda, V, B], [lambda, X, ABB]) :-
    gensym(alpha_,X),
    replace(V, B, X, BB),
    alpha_reduction(BB, ABB).
@
Observe that the only work is done in the last rule.
The runtime-provided [[gensym]] is used to create the next unique atom beginning with the prefix [[alpha_]].
We then replace the \emph{variable} [[V]] with [[X]], and reconstitute our function.
There is no reason why we can't use [[beta_reduction]] on the input function, except that it is unneeded.

We can now define when two $\lambda$-expressions are isomorphic:
<<Isomorphic>>=
canon(L, R) :- reset_gensym(alpha_), alpha_reduction(L, R).
isomorphic(A, B) :- canon(A, C), canon(B, C).
@

\subsection{Demonstrations}
<<AlphaReduction.tests>>=
isomorphic(x, x).
isomorphic([lambda, x, x], [lambda, y, y]).
not(isomorphic([lambda, x, x], [lambda, y, [y, y]])).
not(isomorphic(x, y)).
@
We now have a framework to start building up more familiar math systems!

\section{Implementing Boolean Algebra}
Recall the logic definitions from earlier, in \cref{Logic Definitions}.
<<FirstLoop.filetest>>=
<<Logic Definitions>>
(define nand (lambda x (lambda y (not ((and x) y)))))
(execute ((nand true) true))
(execute ((nand true) false))
(execute ((nand false) true))
(execute ((nand false) false))
(halt)
@

<<SecondLoop.filetest>>=
<<Logic Definitions>>
(define nand (lambda x (lambda y (not ((and x) y)))))
(execute nand)
(execute (nand true))
(execute (lambda x (lambda y (((not nand) x) y))))
(define nnand (lambda x (lambda y (not ((nand x) y)))))
(execute (lambda x (lambda y (not ((nand x) y)))))
(execute ((nnand true) true))
(execute ((nnand true) false))
(execute ((nnand false) true))
(execute ((nnand false) false))
(halt)
@


NOTE: Putting ``not'' in front of ``nand'' evaluates things in the wrong order, things aren't commutative like I'd like.

Now let's show that ``not nand'' is equal to ''and''.

\section{Implementing Arithmetic}
<<Arithmetic>>=
<<Logic Definitions>>
(define cond (lambda e1 (lambda e2 (lambda c ((c e1) e2)))))
(define iszero (lambda n (n true)))
(define zero (lambda s s))
(define succ (lambda n (lambda s ((s false) n))))
(define one (succ zero))
(define pred1 (lambda n (n false)))
@

<<Arithmetic.filetest>>=
<<Arithmetic>>
(execute (succ zero))
(execute (pred1 one))
(halt)
@

<<Addition Demonstration>>=
(define add2 (lambda f (lambda x (lambda y))))

\section{Parsing}
We have been looking at $\lambda$s in an annoying way, as [[[lambda, N, B]]].
It would be nice if we could write that as [[(lambda N B)]], for instance.
We need a parser.

NOTE: This is an example where not all ``sugar'' can be implemented within $\lambda$-calculus.
Obviously here we need I/O hooks, but later on we can't really excise parens except by writing a different parser (which we will not do).

In classic parsing style, we'll implement a tokenizer, and then a grammar.

\subsection{Tokenizer}
Handling input streams is tricky and annoying.
If memory serves (this is me writing in 2020 trying to remember what I did in 2016) I am greatly indebted to Sterling and Shapiro\cite{artofprolog} in designing a prolog-implemented parser.

<<Tokenizer>>=
read_s(S)           :- get_char(C),                    read_s(C, S).
read_s(' ',S)       :- get_char(D),                    read_s(D, S).
read_s('(',['('|S]) :- get_char(D),                    read_s(D, S).
read_s(')',[')'|S]) :- get_char(D),                    read_s(D, S).
read_s(C,[N|S])     :- name_char(C), read_name(C,N,D), read_s(D, S).
read_s('\n',[]).
@
The tokenizer is a driver that reads in new characters, and matches them into tokens: parens, names, or end-of-line.
The clause [[get_char]] is the entry point into the outside world.
Again think of the non-chronological perspective. It's almost like we're building up [[S]].
Observe that we use the \emph{goal} to basically append to [[S]].

The output, in a sense, is [[S]] in the topmost clause.

The variable [[C]] is something like the ``current'' character, and [[D]] is the ``next'' character.
We have to keep these characters in the parameter lists so that we don't accidentally discard them.
Observe, for instance, how [[end_of_line]] is a non-recursive case.

The only ``interesting'' token, then, is that determined by [[read_name]].
The clauses for that predicate are as follows:
<<Read Name>>=
read_name(C,N,E) :-
    read_name_chars(S,C,E),
    atom_chars(N,S).
read_name_chars([C|S], C, E) :-
    name_char(C), !, % needed for io
    get_char(D),
    read_name_chars(S, D, E).
read_name_chars([],C,C) :- not(name_char(C)).

name_char(C) :- char_code(C,N), N >= 65, N =< 90. % upper case
name_char(C) :- char_code(C,N), N >= 97, N =< 122. % lower case.
name_char(C) :- char_code(C,N), N == 95. % underscore
name_char(C) :- char_code(C,N), N >= 48, N =< 57. % decimals
@
The output parameter, such that it is, is [[N]] in the first rule.
That is an atom, resulted by the character [[C]] \emph{prepended} on to the string [[S]] built up by [[read_name_chars]].
The character [[E]] is the evidence that we've stopped reading valid-name-characters (see the last clause of [[read_name_chars]]).

I can't explain the need for [[!]], except the comment included from 2016.

This concludes the tokenizer.

\subsection{Parser}
Now given a sequence of tokens, we can distill them into the nested lists we want for our internal representation.
The tricky thing is matching parentheses.
<<Parser>>=
parse(S,L) :- parse(S, [], [L]).
parse(['('|T], R, [M|L]) :- parse(T, [')'|S], M), parse(S, R, L).
parse([')'|T], [')'|T], []).
parse([N|T], R, [N|L]) :- N \= ')', N \= '(', parse(T, R, L).
parse([],[],[]).
@
How does this work? That's an excellent question.
We shall go line-by-line:
\begin{enumerate}
\item This is the entry point: A parse is valid when the sequence-of-tokens [[S]] is the same as the $\lambda$-expression [[L]].
This is implemented by deferring to [[parse/3]], where the first parameter is some prefix of tokens, the second is the unmatched suffix of tokens, and the third parameter is the $\lambda$-expression of the prefix.
So the entry-point parse is valid when there is no unparsed suffix.
\item This first rule for [[parse/3]] handles when the next token it sees is open-parens.
This is where much of the trickiness happens.
We essentially defer to a sub-parse of the tail [[T]] of that prefix, ending with the matching parens [[')']].
The result of that sub-parse is [[M]], which we wrap in a list (because it was in parens).
The remaining suffix of that sub-parse, [[S]], is itself parsed to give us the \emph{tail} following that [[M]].
\item When we reach the end of a parens, that is the start of some intermediate list we build up, so the result of that parse is [].
\item The base case.
\end{enumerate}

We also want to print out our internal lambda representation the same way.
Note that [[atomic_list_concat]] is provided by our environment, and does what you think.
<<Lambda Printer>>=
lambda_to_atom(L, R) :- parse(S, L), atomic_list_concat(S, ' ', R).
@
Isn't that neat, we use the parser to ``unparse'' the $\lambda$-expression?
We'd like to be able to do that with sugaring/desugaring/$\beta$-reductions, but I've not figured it out yet.

Finally, we want to present a simple user-interactive loop.
Don't call it a repl.
We have a few different commands we can unify against:
<<Repl Commands>>=
compute(L, D, R) :-
    desugar_all(L, D, DL),
    evaluate(DL, S),
    resugar_all(S, D, R).
execute_command(['define', N, L], D, [[N,R]|D]) :- compute(L, D, R).
execute_command(['execute', L], D,D) :-
    desugar_all(L, D, DL),
    evaluate(DL, R),
    resugar_all(R, D, SR),
    lambda_to_atom(SR,O),
    write(O), nl.
execute_command(['trace' | T],D,D) :- trace, execute_command(T, D, D).
execute_command(['halt'],_,_) :- halt.
@

<<Main Loop>>=
main_loop(OD) :-
    read_s(S),
    parse(S,L),
    execute_command(L,OD,ND),
    main_loop(ND).
main_loop(OD) :- write('Parse error'), nl, main_loop(OD).
main :- main_loop([]), halt.
@



<<lambda.pl>>=
<<Initial Lambda Calculus Definitions>>
<<Beta Reduction>>
<<Replace Predicate>>
<<Evaluate>>
<<Alpha Reduction>>
<<Isomorphic>>
<<Desugaring>>
<<Resugaring>>
<<Tokenizer>>
<<Read Name>>
<<Parser>>
<<Lambda Printer>>
<<Repl Commands>>
<<Main Loop>>
@

\section{Extentions}
Resugaring should be able to be expressed as beta-reduction, but backwards.
This may require canonicalization to be done in beta reductions.

Implementing arithmetic and other demonstrations.

Observe that we very often traverse the $\lambda$-expression tree (the repeated pattern of the name being the base case, and the function/applications being recursively mapped).
Perhaps this suggests a second-order predicate that maps, or folds, or both, over the expression.
Maybe this is too close to $\lambda$-calculus and would get us full circle; I don't know.

I wonder if the parser can be improved or reduced in lines-of-code, though I gave it a shot and it's hard to match parens.
I think the original code may have been tracing through a parser provided in \cite{artofprolog}.

\appendix
\newpage
\section{Complete Code Listing}
\inputminted[linenos=true]{prolog}{lambda.pl}
\newpage

\bibliography{library}
\bibliographystyle{alpha}

\end{document}